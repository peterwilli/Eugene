{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name AI\n",
    "=======\n",
    "\n",
    "Name AI is an machine learning application that can come up with brandable domain names.\n",
    "\n",
    "How it works\n",
    "============\n",
    "\n",
    "For more understanding how it works, please refer to my blogpost: [Let AI come up with your next domain name!](https://codebuffet.co/2017/03/31/let-ai-come-up-with-your-next-domain-name/)\n",
    "\n",
    "---\n",
    "\n",
    "Copyright (C) 2017 Peter Willemsen\n",
    "\n",
    "This program is free software: you can redistribute it and/or modify\n",
    "it under the terms of the GNU General Public License as published by\n",
    "the Free Software Foundation, either version 3 of the License, or\n",
    "(at your option) any later version.\n",
    "\n",
    "This program is distributed in the hope that it will be useful,\n",
    "but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "GNU General Public License for more details.\n",
    "\n",
    "You should have received a copy of the GNU General Public License\n",
    "along with this program.  If not, see <http://www.gnu.org/licenses/>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'theano'\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Activation, Dropout\n",
    "from keras.layers import GRU\n",
    "from keras.regularizers import l2, activity_l2\n",
    "from keras.optimizers import RMSprop\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "import math\n",
    "from datetime import datetime\n",
    "import time\n",
    "import random\n",
    "import sys\n",
    "import string\n",
    "import h5py\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed = 80085\n",
    "np.random.seed(seed)  # for reproducibility\n",
    "random.seed(seed)\n",
    "weights_path = './model.hdf5'\n",
    "\n",
    "csv.register_dialect('eugene', delimiter=';', quotechar='\"', quoting=csv.QUOTE_MINIMAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create character indices, so that each character is linked to a number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total chars: 48, maxlen2: 253\n"
     ]
    }
   ],
   "source": [
    "mask = '_'\n",
    "chars = None\n",
    "maxlen = 3\n",
    "domains = []\n",
    "next_chars = []\n",
    "maxlen2 = 0\n",
    "\n",
    "with open('../syntethic_data/data.csv', 'r') as f:\n",
    "    reader = csv.reader(f, 'eugene')\n",
    "    text = \"\"\n",
    "    for row in reader:\n",
    "        to_append = row[0] + \":\" + row[1]\n",
    "        text += to_append\n",
    "        maxlen2 = max(maxlen2, len(to_append))\n",
    "    chars = sorted(list(set(text + mask)))\n",
    "    \n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "print(('total chars: %d, maxlen2: %d' % (len(chars), maxlen2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pad_name(name, max_length, skip = 0):\n",
    "    padded_name = mask * (max_length - len(name) - skip)\n",
    "    padded_name += name\n",
    "    \n",
    "    return padded_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We count the longest domain name in the corpus. This length will be used to scale the rest of the data, so that no domain will be cut off. The rest will be left-padded with '_'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put the data in a time-series window of 3 steps. A window of 3 characters means that the network will keep in mind the past 3 characters, while predicting the next single character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> c\n",
      "__c -> r\n",
      "_cr -> e\n",
      "cre -> a\n",
      "rea -> t\n",
      "eat -> e\n",
      "ate ->  \n",
      "te  -> 1\n",
      "e 1 ->  \n",
      " 1  -> d\n",
      "1 d -> i\n",
      " di -> r\n",
      "dir -> e\n",
      "ire -> c\n",
      "rec -> t\n",
      "ect -> o\n",
      "cto -> r\n",
      "tor -> i\n",
      "ori -> e\n",
      "rie -> s\n",
      "ies ->  \n",
      "es  -> s\n",
      "s s -> t\n",
      " st -> a\n",
      "sta -> r\n",
      "tar -> t\n",
      "art -> i\n",
      "rti -> n\n",
      "tin -> g\n",
      "ing ->  \n",
      "ng  -> w\n",
      "g w -> i\n",
      " wi -> t\n",
      "wit -> h\n",
      "ith ->  \n",
      "th  -> O\n",
      "h O -> 4\n",
      " O4 -> S\n",
      "O4S -> C\n",
      "4SC -> X\n",
      "SCX -> P\n",
      "CXP -> K\n",
      "XPK -> E\n",
      "PKE -> U\n",
      "KEU -> :\n",
      "EU: -> m\n",
      "U:m -> k\n",
      ":mk -> d\n",
      "mkd -> i\n",
      "kdi -> r\n",
      "dir ->  \n",
      "ir  -> O\n",
      "r O -> 4\n",
      " O4 -> S\n",
      "O4S -> C\n",
      "4SC -> X\n",
      "SCX -> P\n",
      "CXP -> K\n",
      "XPK -> E\n",
      "PKE -> U\n",
      "KEU -> _\n",
      "EU_ -> 1\n",
      "U_1 -> _\n",
      "_1_ -> _\n",
      "1__ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> _\n",
      "___ -> c\n",
      "__c -> r\n",
      "_cr -> e\n",
      "cre -> a\n",
      "rea -> t\n",
      "eat -> e\n",
      "ate ->  \n",
      "te  -> 2\n",
      "e 2 ->  \n",
      " 2  -> d\n",
      "2 d -> i\n",
      " di -> r\n",
      "dir -> e\n",
      "ire -> c\n",
      "rec -> t\n",
      "ect -> o\n",
      "cto -> r\n",
      "tor -> i\n",
      "ori -> e\n",
      "rie -> s\n",
      "ies ->  \n",
      "es  -> s\n",
      "s s -> t\n",
      " st -> a\n",
      "sta -> r\n",
      "tar -> t\n",
      "art -> i\n",
      "rti -> n\n",
      "tin -> g\n",
      "ing ->  \n",
      "ng  -> w\n",
      "g w -> i\n",
      " wi -> t\n",
      "wit -> h\n",
      "ith ->  \n",
      "th  -> O\n",
      "h O -> 4\n",
      " O4 -> S\n",
      "O4S -> C\n",
      "4SC -> X\n",
      "SCX -> P\n",
      "CXP -> K\n",
      "XPK -> E\n",
      "PKE -> U\n",
      "KEU -> :\n",
      "EU: -> m\n",
      "U:m -> k\n",
      ":mk -> d\n",
      "mkd -> i\n",
      "kdi -> r\n",
      "dir ->  \n",
      "ir  -> O\n",
      "r O -> 4\n",
      " O4 -> S\n",
      "O4S -> C\n",
      "4SC -> X\n",
      "SCX -> P\n",
      "CXP -> K\n",
      "XPK -> E\n",
      "PKE -> U\n",
      "KEU -> _\n",
      "EU_ -> 1\n",
      "U_1 ->  \n",
      "_1  -> &\n",
      "1 & -> &\n",
      " && ->  \n",
      "&&  -> m\n",
      "& m -> k\n",
      " mk -> d\n",
      "mkd -> i\n",
      "kdi -> r\n",
      "dir ->  \n",
      "ir  -> O\n",
      "r O -> 4\n",
      " O4 -> S\n",
      "O4S -> C\n",
      "4SC -> X\n",
      "SCX -> P\n",
      "CXP -> K\n",
      "XPK -> E\n",
      "PKE -> U\n",
      "KEU -> _\n",
      "EU_ -> 2\n",
      "U_2 -> _\n",
      "_2_ -> _\n",
      "2__ -> _\n",
      "nb sequences: 25297 25297\n",
      "batch size: 25300 maxlen2: 253\n"
     ]
    }
   ],
   "source": [
    "step = 1\n",
    "batch_size = 512\n",
    "with open('../syntethic_data/data.csv', 'r') as f:\n",
    "    padded_names = \"\"\n",
    "    reader = csv.reader(f, 'eugene')\n",
    "    for row in reader:\n",
    "        line = row[0] + \":\" + row[1]\n",
    "        padded_names += pad_name(line, maxlen2)\n",
    "\n",
    "for i in range(0, len(padded_names) - maxlen, step):\n",
    "    domains.append(padded_names[i: i + maxlen])\n",
    "    next_chars.append(padded_names[i + maxlen])\n",
    "\n",
    "for i in range(0, maxlen2*2):\n",
    "    print(\"%s -> %s\" % (domains[i], next_chars[i]))\n",
    "\n",
    "print('nb sequences:', len(domains), len(next_chars))\n",
    "print(\"batch size: %d maxlen2: %d\" % (batch_size, maxlen2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorization - convert the time-series window from a list of characters to a list of numbers (the character embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.zeros((len(domains), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(domains), len(chars)), dtype=np.bool)\n",
    "for i, domain in enumerate(domains):\n",
    "    for t, char in enumerate(domain):\n",
    "        X[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model consists of a single GRU layer with 128 units (which is quite small). My goal was that Name AI would learn the linguistics of what makes a name brandable, in which case such a small model is a perfect fit. \n",
    "\n",
    "This model has a small dropout to make sure that we don't jolt our loss down to near-zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(GRU(128, dropout_W=0.015, dropout_U=0.015, input_shape=(maxlen, len(chars)), return_sequences=False, stateful=False))\n",
    "\n",
    "#for a hidden layer, uncomment this one\n",
    "#model.add(GRU(128, dropout_W=0.015, dropout_U=0.015, return_sequences=False, stateful=False))\n",
    "model.add(Dense(len(chars)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(weights_path):\n",
    "    print(\"loading existing model..\")\n",
    "    model.load_weights(weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generate_alphabet_names():\n",
    "    seeds = string.ascii_lowercase\n",
    "    generate_names_amount = len(seeds)\n",
    "    diversity = random.uniform(0.05, 0.5)\n",
    "\n",
    "    print(\"Name AI by Peter Willemsen <peter@codebuffet.co>\\nCreating %d names with diversity %f\" % (generate_names_amount, diversity))\n",
    "    for i in range(0, generate_names_amount):\n",
    "        seed = pad_name(seeds[i], maxlen)\n",
    "        sentence = seed\n",
    "        generated = seed\n",
    "        domains = generated\n",
    "\n",
    "        for i in range(maxlen2 * 1):\n",
    "            x = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            domains += next_char\n",
    "        print(domains.replace(\"_\", \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model, output generated text after each iteration. We make sure we don't shuffle our data, otherwise the order of the 3-step window will be broken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name AI by Peter Willemsen <peter@codebuffet.co>\n",
      "Creating 26 names with diversity 0.275346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO (theano.gof.compilelock): Refreshing lock /home/peter/.theano/compiledir_Linux-4.8--generic-x86_64-with-Ubuntu-16.04-xenial-x86_64-3.5.2-64/lock_dir/lock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "an\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'b'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-104c860109b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mclear_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mgenerate_alphabet_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0miteration\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-a789d2e86a04>\u001b[0m in \u001b[0;36mgenerate_alphabet_names\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m                 \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'b'"
     ]
    }
   ],
   "source": [
    "iteration = 0\n",
    "while True:\n",
    "    print('Iteration', iteration)\n",
    "    model.fit(X, y, batch_size=batch_size, shuffle=False, nb_epoch=1)\n",
    "    model.save_weights(weights_path, overwrite=True)\n",
    "    clear_output()    \n",
    "    generate_alphabet_names()      \n",
    "        \n",
    "    iteration += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "generate_alphabet_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
